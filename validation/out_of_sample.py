"""
Module de validation out-of-sample pour QAAF.
Permet d'√©valuer la robustesse des param√®tres optimis√©s sur des donn√©es non utilis√©es
pour l'optimisation.
"""

import pandas as pd
import numpy as np
import logging
import matplotlib.pyplot as plt
from typing import Dict,List,Tuple,Any,Optional,Callable
from datetime import datetime

# Configuration du logging
logger=logging.getLogger (__name__)


class OutOfSampleValidator:
    """
    Validateur out-of-sample pour QAAF.
    √âvalue la performance des param√®tres optimis√©s sur des donn√©es hors √©chantillon.
    """

    def __init__ (self,
                  data: Dict[str,pd.DataFrame],
                  backtest_function: Callable,
                  metric_keys: List[str] = None):
        """
        Initialise le validateur out-of-sample.

        Args:
            data: Dictionnaire des donn√©es pour la validation (BTC, PAXG, PAXG/BTC)
            backtest_function: Fonction de backtest prenant des donn√©es et des param√®tres
            metric_keys: Liste des m√©triques √† analyser pour la validation
        """
        self.data=data
        self.backtest_function=backtest_function
        self.metric_keys=metric_keys or ['total_return','max_drawdown','sharpe_ratio','volatility']
        self.results={}

    def split_data (self,
                    train_ratio: float = 0.7,
                    validation_ratio: float = 0.0,
                    test_ratio: float = 0.3) -> Dict[str,Dict[str,pd.DataFrame]]:
        """
        Divise les donn√©es en ensembles d'entra√Ænement, validation et test.

        Args:
            train_ratio: Proportion des donn√©es pour l'entra√Ænement
            validation_ratio: Proportion des donn√©es pour la validation (0 pour n'utiliser que train/test)
            test_ratio: Proportion des donn√©es pour le test

        Returns:
            Dictionnaire des ensembles de donn√©es
        """
        # V√©rification que les ratios somment √† 1
        total_ratio=train_ratio + validation_ratio + test_ratio
        if abs (total_ratio - 1.0) > 1e-10:
            logger.warning (f"Les ratios somment √† {total_ratio}, normalisation appliqu√©e")
            train_ratio/=total_ratio
            validation_ratio/=total_ratio
            test_ratio/=total_ratio

        # R√©cup√©ration des dates communes
        common_dates=self._get_common_dates ()

        # Calcul des indices pour la division
        n_total=len (common_dates)
        n_train=int (n_total * train_ratio)
        n_validation=int (n_total * validation_ratio)

        train_end_idx=n_train - 1
        validation_end_idx=n_train + n_validation - 1

        train_end_date=common_dates[train_end_idx]
        validation_end_date=common_dates[validation_end_idx] if validation_ratio > 0 else train_end_date

        # Division des donn√©es
        result={}

        # Ensemble d'entra√Ænement
        train_data={}
        for asset,df in self.data.items ():
            train_mask=df.index <= train_end_date
            train_data[asset]=df[train_mask].copy ()
        result['train']=train_data

        # Ensemble de validation (si demand√©)
        if validation_ratio > 0:
            validation_data={}
            for asset,df in self.data.items ():
                validation_mask=(df.index > train_end_date) & (df.index <= validation_end_date)
                validation_data[asset]=df[validation_mask].copy ()
            result['validation']=validation_data

        # Ensemble de test
        test_data={}
        for asset,df in self.data.items ():
            test_mask=df.index > validation_end_date
            test_data[asset]=df[test_mask].copy ()
        result['test']=test_data

        # Stockage des r√©sultats
        self.data_splits=result

        # Logging
        logger.info (f"Division des donn√©es: Train jusqu'au {train_end_date.strftime ('%Y-%m-%d')}")
        if validation_ratio > 0:
            logger.info (
                f"Validation du {train_end_date.strftime ('%Y-%m-%d')} au {validation_end_date.strftime ('%Y-%m-%d')}")
        logger.info (f"Test du {validation_end_date.strftime ('%Y-%m-%d')} √† la fin")

        return result

    def _get_common_dates (self) -> List[pd.Timestamp]:
        """
        R√©cup√®re les dates communes √† tous les DataFrames.

        Returns:
            Liste des dates communes
        """
        if not self.data:
            return []

        # Initialisation avec les dates du premier DataFrame
        first_key=next (iter (self.data))
        common_dates=set (self.data[first_key].index)

        # Intersection avec les dates des autres DataFrames
        for asset,df in self.data.items ():
            if asset != first_key:
                common_dates&=set (df.index)

        return sorted (list (common_dates))

    def run_validation (self,
                        train_params: Dict[str,Any],
                        train_ratio: float = 0.7,
                        test_ratio: float = 0.3) -> Dict:
        """
        Ex√©cute la validation out-of-sample avec des param√®tres donn√©s.

        Args:
            train_params: Param√®tres optimis√©s sur les donn√©es d'entra√Ænement
            train_ratio: Proportion des donn√©es pour l'entra√Ænement
            test_ratio: Proportion des donn√©es pour le test

        Returns:
            R√©sultats de la validation
        """
        # Division des donn√©es
        self.split_data (train_ratio=train_ratio,test_ratio=test_ratio)

        # Ex√©cution du backtest sur les donn√©es d'entra√Ænement
        logger.info ("Ex√©cution du backtest sur les donn√©es d'entra√Ænement...")
        train_results=self.backtest_function (self.data_splits['train'],train_params)

        # Ex√©cution du backtest sur les donn√©es de test
        logger.info ("Ex√©cution du backtest sur les donn√©es de test...")
        test_results=self.backtest_function (self.data_splits['test'],train_params)

        # Analyse des r√©sultats
        validation_analysis=self.analyze_validation_results (train_results,test_results)

        # Stockage des r√©sultats
        self.results={
            'train':train_results,
            'test':test_results,
            'analysis':validation_analysis
        }

        return self.results

    def analyze_validation_results (self,
                                    train_results: Dict,
                                    test_results: Dict) -> Dict:
        """
        Analyse les r√©sultats de la validation out-of-sample.

        Args:
            train_results: R√©sultats sur les donn√©es d'entra√Ænement
            test_results: R√©sultats sur les donn√©es de test

        Returns:
            Analyse des r√©sultats
        """
        analysis={}

        # Pour chaque m√©trique d'int√©r√™t
        for metric in self.metric_keys:
            if metric not in train_results or metric not in test_results:
                continue

            train_value=train_results[metric]
            test_value=test_results[metric]

            # Calcul du ratio test/train ou train/test selon la m√©trique
            if metric == 'max_drawdown':
                # Pour le drawdown (valeur n√©gative), le ratio est invers√©
                ratio=train_value / test_value if test_value != 0 else float ('inf')
            else:
                ratio=test_value / train_value if train_value != 0 else float ('inf')

            # Stockage des r√©sultats
            analysis[metric]={
                'train':train_value,
                'test':test_value,
                'ratio':ratio,
                'consistency':self._evaluate_consistency (ratio,metric)
            }

        # Score global de robustesse
        robustness_scores=[
            analysis[m]['consistency']
            for m in analysis.keys ()
            if 'consistency' in analysis[m]
        ]

        analysis['overall_robustness']=np.mean (robustness_scores) if robustness_scores else 0.0

        return analysis

    def _evaluate_consistency (self,ratio: float,metric: str) -> float:
        """
        √âvalue la consistance entre les r√©sultats train et test.

        Args:
            ratio: Ratio des valeurs test/train
            metric: Nom de la m√©trique √©valu√©e

        Returns:
            Score de consistance (0-1)
        """
        # Pour le rendement, volatilit√©, Sharpe
        if metric in ['total_return','sharpe_ratio']:
            # Id√©alement ratio proche de 1 (performance similaire)
            if ratio > 1.5 or ratio < 0.5:
                return 0.0  # Tr√®s inconsistant
            elif ratio > 1.2 or ratio < 0.8:
                return 0.5  # Mod√©r√©ment inconsistant
            else:
                return 1.0  # Consistant

        # Pour le drawdown (d√©j√† trait√© avec ratio invers√©)
        elif metric == 'max_drawdown':
            if ratio > 1.5 or ratio < 0.5:
                return 0.0  # Tr√®s inconsistant
            elif ratio > 1.2 or ratio < 0.8:
                return 0.5  # Mod√©r√©ment inconsistant
            else:
                return 1.0  # Consistant

        # Pour d'autres m√©triques
        else:
            # Approche g√©n√©rique
            return max (0.0,min (1.0,1.0 - abs (ratio - 1.0)))

    def plot_validation_results (self) -> None:
        """
        Affiche une visualisation des r√©sultats de validation.
        """
        if not self.results:
            logger.warning ("Aucun r√©sultat de validation disponible √† visualiser")
            return

        # Extraction des donn√©es de performance
        train_perf=self.results['train'].get ('portfolio_values',pd.Series ())
        test_perf=self.results['test'].get ('portfolio_values',pd.Series ())

        # Si les donn√©es de performances ne sont pas disponibles
        if train_perf.empty or test_perf.empty:
            logger.warning ("Donn√©es de performance insuffisantes pour la visualisation")
            return

        # Normalisation pour la comparaison
        norm_train=train_perf / train_perf.iloc[0]
        norm_test=test_perf / test_perf.iloc[0]

        # Cr√©ation de la figure
        plt.figure (figsize=(15,10))

        # Plot des performances
        plt.subplot (2,1,1)
        plt.plot (norm_train.index,norm_train,'b-',label='Entra√Ænement')
        plt.plot (norm_test.index,norm_test,'r-',label='Test')
        plt.title ('Performance Out-of-Sample')
        plt.ylabel ('Performance normalis√©e')
        plt.legend ()
        plt.grid (True)

        # Plot des m√©triques de validation
        plt.subplot (2,1,2)
        metrics=list (self.results['analysis'].keys ())
        metrics=[m for m in metrics if m != 'overall_robustness']

        if metrics:
            # Barres group√©es pour train et test
            x=np.arange (len (metrics))
            width=0.35

            train_values=[self.results['analysis'][m]['train'] for m in metrics]
            test_values=[self.results['analysis'][m]['test'] for m in metrics]

            # Ajustement des valeurs pour l'affichage (drawdown est n√©gatif)
            for i,metric in enumerate (metrics):
                if metric == 'max_drawdown':
                    train_values[i]=-train_values[i]
                    test_values[i]=-test_values[i]

            plt.bar (x - width / 2,train_values,width,label='Entra√Ænement')
            plt.bar (x + width / 2,test_values,width,label='Test')

            plt.xlabel ('M√©trique')
            plt.ylabel ('Valeur')
            plt.title ('Comparaison des m√©triques')
            plt.xticks (x,metrics)
            plt.legend ()
            plt.grid (True,axis='y')

            # Ajout des valeurs sur les barres
            for i,v in enumerate (train_values):
                plt.text (i - width / 2,v + 0.02,f'{v:.2f}',ha='center')
            for i,v in enumerate (test_values):
                plt.text (i + width / 2,v + 0.02,f'{v:.2f}',ha='center')

        plt.tight_layout ()
        plt.show ()

    def generate_validation_report (self) -> str:
        """
        G√©n√®re un rapport textuel des r√©sultats de validation.

        Returns:
            Rapport format√©
        """
        if not self.results:
            return "Aucun r√©sultat de validation disponible."

        report="# Rapport de Validation Out-of-Sample\n\n"
        report+=f"Date: {datetime.now ().strftime ('%Y-%m-%d %H:%M:%S')}\n\n"

        # Donn√©es g√©n√©rales
        train_start=min (self.data_splits['train']['BTC'].index)
        train_end=max (self.data_splits['train']['BTC'].index)
        test_start=min (self.data_splits['test']['BTC'].index)
        test_end=max (self.data_splits['test']['BTC'].index)

        report+=f"## P√©riodes d'analyse\n"
        report+=f"- **Entra√Ænement:** {train_start.strftime ('%Y-%m-%d')} - {train_end.strftime ('%Y-%m-%d')}\n"
        report+=f"- **Test:** {test_start.strftime ('%Y-%m-%d')} - {test_end.strftime ('%Y-%m-%d')}\n\n"

        # R√©sultats des m√©triques
        report+="## Comparaison des m√©triques\n\n"
        report+="| M√©trique | Entra√Ænement | Test | Ratio | Consistance |\n"
        report+="|----------|--------------|------|-------|------------|\n"

        for metric in self.metric_keys:
            if metric not in self.results['analysis']:
                continue

            analysis=self.results['analysis'][metric]
            train_val=analysis['train']
            test_val=analysis['test']
            ratio=analysis['ratio']
            consistency=analysis['consistency']

            # Formatage des valeurs
            train_str=f"{train_val:.2f}" if isinstance (train_val,(int,float)) else str (train_val)
            test_str=f"{test_val:.2f}" if isinstance (test_val,(int,float)) else str (test_val)
            ratio_str=f"{ratio:.2f}" if isinstance (ratio,(int,float)) else str (ratio)

            # √âvaluation de la consistance
            if consistency >= 0.8:
                cons_str="‚úÖ Excellente"
            elif consistency >= 0.5:
                cons_str="‚ö†Ô∏è Mod√©r√©e"
            else:
                cons_str="‚ùå Faible"

            report+=f"| {metric} | {train_str} | {test_str} | {ratio_str} | {cons_str} |\n"

        # Score global de robustesse
        robustness=self.results['analysis'].get ('overall_robustness',0.0)
        report+=f"\n## √âvaluation globale\n\n"
        report+=f"Score de robustesse: {robustness:.2f}/1.00\n\n"

        if robustness >= 0.8:
            report+="üìä **Analyse**: Les param√®tres montrent une excellente robustesse sur les donn√©es de test.\n"
        elif robustness >= 0.5:
            report+="üìä **Analyse**: Les param√®tres montrent une robustesse mod√©r√©e, acceptable pour un d√©ploiement prudent.\n"
        else:
            report+="üìä **Analyse**: Les param√®tres montrent une faible robustesse, sugg√©rant un possible surajustement aux donn√©es d'entra√Ænement.\n"

        return report

    def validate_strategy (backtest_function: Callable,
                           data: Dict[str,pd.DataFrame],
                           params: Dict[str,Any],
                           train_ratio: float = 0.7,
                           test_ratio: float = 0.3) -> Dict:
        """
        Fonction utilitaire pour valider rapidement une strat√©gie.

        Args:
            backtest_function: Fonction de backtest
            data: Donn√©es de march√©
            params: Param√®tres √† valider
            train_ratio: Proportion des donn√©es pour l'entra√Ænement
            test_ratio: Proportion des donn√©es pour le test

        Returns:
            R√©sultats de validation
        """
        validator=OutOfSampleValidator (data,backtest_function)
        return validator.run_validation (params,train_ratio,test_ratio)